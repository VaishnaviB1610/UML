{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oX6tRjnnQEe"
   },
   "source": [
    "#Step 1. Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWlrm5qfXLuQ",
    "outputId": "d47a491a-48f9-4d59-dc3e-177e73a4f194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_mnist_keras():\n",
    "    # Load MNIST dataset from TensorFlow/Keras\n",
    "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # latten the images to shape (num_samples, 784)\n",
    "    train_images = train_images.reshape(-1, 28*28).astype('float32')\n",
    "    test_images = test_images.reshape(-1, 28*28).astype('float32')\n",
    "\n",
    "    #Normalize images to the range [0, 1]\n",
    "    train_images /= 255.0\n",
    "    test_images /= 255.0\n",
    "\n",
    "    return (train_images, train_labels), (test_images, test_labels)\n",
    "\n",
    "#Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = load_mnist_keras()\n",
    "\n",
    "#Print the shape to verify\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_jU8gvMXPCX",
    "outputId": "a980e588-bb47-4fd8-e2ec-3a6244bb2a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n"
     ]
    }
   ],
   "source": [
    "#20 NG Dataset\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Load 20 Newsgroups data\n",
    "def load_20ng():\n",
    "    newsgroups = fetch_20newsgroups(subset='all')  # Load both training and test data\n",
    "    return newsgroups.data, newsgroups.target\n",
    "\n",
    "text_data, labels = load_20ng()\n",
    "print(len(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y181UBLmbfBb",
    "outputId": "27b2bc19-c4b1-431b-f933-ec389269ea55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vaishnavibhutada/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vaishnavibhutada/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vaishnavibhutada/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF matrix: (18846, 93622)\n",
      "Vocabulary size: 93622\n",
      "\n",
      "Sample features: ['aa' 'aaa' 'aaaa' 'aaaaa' 'aaaaaaaaaaaa'\n",
      " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaauuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuugggggggggggggggg'\n",
      " 'aaaaagggghhhh' 'aaaaarrrrgh' 'aaaah' 'aaaahhh']\n"
     ]
    }
   ],
   "source": [
    "#Parsing 20 NG Dataset\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#Downloaded necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "#Tokenize the text and remove stopwords\n",
    "def tokenize_and_remove_stopwords(text_data):\n",
    "    stop_words = set(stopwords.words('english'))  #List of common stopwords\n",
    "    tokenized_docs = []\n",
    "\n",
    "    for doc in text_data:\n",
    "        #Tokenize the document\n",
    "        tokens = nltk.word_tokenize(doc.lower())  #Convert to lowercase and tokenize\n",
    "        #Remove stopwords and non-alphabetical tokens\n",
    "        filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "        tokenized_docs.append(' '.join(filtered_tokens))  #Join back to form a clean document string\n",
    "\n",
    "    return tokenized_docs\n",
    "\n",
    "#Vectorize the text\n",
    "def vectorize_text(tokenized_docs, method='tf-idf'):\n",
    "    if method == 'tf':\n",
    "        vectorizer = CountVectorizer()  #Counts term frequencies\n",
    "    elif method == 'tf-idf':\n",
    "        vectorizer = CountVectorizer()\n",
    "        tf_matrix = vectorizer.fit_transform(tokenized_docs)\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        tfidf_matrix = tfidf_transformer.fit_transform(tf_matrix)  #Transform to TF-IDF\n",
    "        return tfidf_matrix, vectorizer\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'tf' or 'tf-idf'\")\n",
    "\n",
    "    return vectorizer.fit_transform(tokenized_docs)  #Term-Document Matrix (TDM) for TF\n",
    "\n",
    "# Parse the 20 Newsgroups dataset\n",
    "def parse_20ng(text_data):\n",
    "    #Tokenize and remove stopwords\n",
    "    tokenized_data = tokenize_and_remove_stopwords(text_data)\n",
    "\n",
    "    #Vectorize the text using TF-IDF\n",
    "    tfidf_matrix, vectorizer = vectorize_text(tokenized_data, method='tf-idf')\n",
    "\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# Example usage for 20 Newsgroups\n",
    "tfidf_matrix, vectorizer = parse_20ng(text_data)\n",
    "\n",
    "# Verify the shape of the term-document matrix\n",
    "print(f\"Shape of the TF-IDF matrix: {tfidf_matrix.shape}\")  # (num_documents, num_terms)\n",
    "\n",
    "#To see the feature names\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "print()\n",
    "print(f\"Sample features: {vectorizer.get_feature_names_out()[:10]}\")  #Print first 10 words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVFXRDqh0n44"
   },
   "source": [
    "# Step 2. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPYuwrq2eGOk"
   },
   "source": [
    "The MNIST images are already in a range of [0, 1], which means they have already been shifted and scaled. However, we can perform zero mean, unit variance normalization for better consistency.\n",
    "\n",
    "Zero mean, unit variance normalization involves:\n",
    "\n",
    "Subtracting the mean of each pixel across all images.\n",
    "Dividing by the standard deviation to scale the pixel values to a variance of 1.\n",
    "For MNIST, we'll apply this normalization to each pixel across all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9OEAv7mKeE4e",
    "outputId": "dbd63d43-acfb-4a14-9e57-03e6f8f81cd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of normalized train images: -5.346976195141906e-06\n",
      "Std of normalized train images: 0.956339955329895\n"
     ]
    }
   ],
   "source": [
    "def normalize_mnist(images):\n",
    "    #Zero mean, unit variance normalization\n",
    "    mean = np.mean(images, axis=0)\n",
    "    std = np.std(images, axis=0)\n",
    "\n",
    "    #Prevent division by zero by replacing 0 standard deviation with 1 (no scaling)\n",
    "    std[std == 0] = 1\n",
    "\n",
    "    #Normalize images\n",
    "    normalized_images = (images - mean) / std\n",
    "    return normalized_images\n",
    "\n",
    "#Normalize the training and test images\n",
    "train_images_normalized = normalize_mnist(train_images)\n",
    "test_images_normalized = normalize_mnist(test_images)\n",
    "\n",
    "#Check the results\n",
    "print(f\"Mean of normalized train images: {np.mean(train_images_normalized)}\")\n",
    "print(f\"Std of normalized train images: {np.std(train_images_normalized)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIyPoKIPeyVO"
   },
   "source": [
    "For the 20 Newsgroups datas- Term Frequency (TF) normalization.\n",
    "\n",
    "The TF normalization involves:\n",
    "\n",
    "Mapping each term to its frequency in the document (using CountVectorizer from sklearn).\n",
    "TF normalization will ensure the sum of the term frequencies in each document is 1. This is important for comparing the relative importance of terms across documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojBVmPKZdhXQ",
    "outputId": "5e66a1d7-72d9-4c8b-bf7f-2cac8cb5308d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF matrix: (18846, 173762)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "def normalize_20ng(text_data):\n",
    "    #Tokenize and vectorize text data (CountVectorizer)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    tf_matrix = count_vectorizer.fit_transform(text_data)\n",
    "\n",
    "    #Apply TF-IDF normalization\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf_matrix = tfidf_transformer.fit_transform(tf_matrix)  # Normalize TF to TF-IDF\n",
    "\n",
    "    return tfidf_matrix, count_vectorizer\n",
    "\n",
    "#Normalize the 20 Newsgroups text data\n",
    "tfidf_matrix, count_vectorizer = normalize_20ng(text_data)\n",
    "\n",
    "#Check the shape of the resulting normalized TF-IDF matrix\n",
    "print(f\"Shape of the TF-IDF matrix: {tfidf_matrix.shape}\")  # (num_documents, num_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlgMfXZD3dzS"
   },
   "source": [
    "# Step 3. Pairwise similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSjXN-CricD1"
   },
   "source": [
    "For MNIST:\n",
    "\n",
    "Using a Library with Batch Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLhEb9lAd8GM",
    "outputId": "c5997ba4-a01d-4b2a-bac4-052933c44025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Euclidean distance matrix (batch computation): (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "def compute_distances_in_batches(data, batch_size=1000):\n",
    "    num_samples = data.shape[0]\n",
    "    distance_matrix = np.zeros((num_samples, num_samples), dtype=np.float32)\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        end_i = min(i + batch_size, num_samples)\n",
    "        distances_batch = cdist(data[i:end_i], data, metric='euclidean')\n",
    "        distance_matrix[i:end_i, :] = distances_batch\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "#Compute the pairwise distance matrix in batches\n",
    "batch_size = 1000\n",
    "train_subset = train_images_normalized[:5000]\n",
    "distance_matrix = compute_distances_in_batches(train_subset, batch_size=batch_size)\n",
    "\n",
    "#Print the shape of the resulting matrix\n",
    "print(f\"Shape of the Euclidean distance matrix (batch computation): {distance_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8dxJYHl4L03"
   },
   "source": [
    "Euclidean Distance Implementation from Scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApQOUCHSiNlE",
    "outputId": "f8d67711-4faf-45c2-d715-ece2ce0d2a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Euclidean distance matrix: (100, 100)\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "#Computing pairwise Euclidean distances between the first 100 images to avoid memory overload\n",
    "num_samples = 100\n",
    "euclidean_distances_batch = np.zeros((num_samples, num_samples))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    for j in range(num_samples):\n",
    "        euclidean_distances_batch[i, j] = euclidean_distance(train_images_normalized[i], train_images_normalized[j])\n",
    "\n",
    "print(f\"Shape of the Euclidean distance matrix: {euclidean_distances_batch.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dbSRpLZn63L"
   },
   "source": [
    "For 20 NG Dataset:\n",
    "\n",
    "Cosine Similarity using a Library (sklearn.metrics.pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWFgw0xdn-QP",
    "outputId": "ba8ea6f2-3144-4bcc-e47b-939797ef3a47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the cosine similarity matrix: (18846, 18846)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Compute pairwise cosine similarities between the documents\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "#Print the shape of the similarity matrix\n",
    "print(f\"Shape of the cosine similarity matrix: {cosine_similarities.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzo_s-P7oUZ_",
    "outputId": "8ac45378-5d98-4c18-e54a-7dfcb94f9085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the cosine similarity matrix (batch): (18846, 18846)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load 20 Newsgroups data\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "text_data = newsgroups.data\n",
    "\n",
    "#Create the TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # Use 5000 most frequent terms\n",
    "tfidf_matrix = vectorizer.fit_transform(text_data)\n",
    "\n",
    "#Function for computing cosine similarity in batches\n",
    "def cosine_similarity_batch(tfidf_matrix, batch_size=300):\n",
    "    num_samples = tfidf_matrix.shape[0]\n",
    "    cosine_similarities_batch = np.zeros((num_samples, num_samples))\n",
    "\n",
    "    #Process the documents in smaller batches to reduce memory usage\n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        batch_data = tfidf_matrix[start_idx:end_idx].toarray()  # Convert batch to dense array\n",
    "\n",
    "        #Compute cosine similarity between the batch and all other documents\n",
    "        cosine_similarities_batch[start_idx:end_idx, :] = cosine_similarity(batch_data, tfidf_matrix.toarray())\n",
    "\n",
    "    return cosine_similarities_batch\n",
    "\n",
    "#Compute cosine similarity matrix in batches\n",
    "cosine_similarities_batch = cosine_similarity_batch(tfidf_matrix, batch_size=500)\n",
    "\n",
    "#Print the shape of the cosine similarity matrix\n",
    "print(f\"Shape of the cosine similarity matrix (batch): {cosine_similarities_batch.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9avOnrMoh-s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
